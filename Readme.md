# FTP客户端与服务器的实现

### FTP客户端

1.创建FTP客户端

- 创建客户端套接字
- 在struct sockaddr_in结构体中填写协议族，远端IP地址，远端端口号(21)
- 发起connect请求，连接服务器
- 接收服务器的信息
- 回应服务器发来的信息

















### FTP服务器

1.创建FTP服务器

- 创建服务器套接字

- 在struct sockaddr_in结构体中填写协议族，本地IP地址(INADDR_ANY)，远端端口号(21)

- 调用bind()函数为服务器套接字绑定struct sockaddr_in结构体信息

- 调用listen()函数将服务器套接口置为被动模式

- 调用signal和waitpid函数处理僵尸子进程(如果服务器是单线程服务器则不需要处理僵死进程)





-------------



### epoll的内部实现原理

https://blog.csdn.net/songchuwang1868/article/details/89877739

1.从网卡接收数据说起

- 网卡会把接收到的数据写入内存

2.如何知道接收了数据

- 网卡会向CPU发出一个中断信号，操作系统便能得知有新数据到来，再通过网卡中断程序去处理数据

3.进程阻塞为什么不占用cpu资源

- 阻塞是进程调度的关键一环，指的是进程在等待某事件(如接收到网络数据)发生之前的等待状态，recv，select和epoll都是阻塞方法。
- 计算机运行着A，B，C三个进程
- 当进程A执行到创建socket的语句时，操作系统会创建一个文件管理的socket对象。这个socket对象包含了发送缓冲区，接收缓冲区，等待队列等成员。等待队列是个非常重要的结构，它指向所有需要等待该socket事件的进程。
- 当程序执行到recv时，操作系统会将进程A从工作队列移动到该sooket的等待队列中。由于工作队列只剩下了进程B和C，依据进程调度，cpu会轮流执行这两个进程的程序，不会执行进程A的程序。所以进程A被阻塞，不会往下执行代码，也不会占用cpu资源。
- 当socket接收到数据后，操作系统将该socket的等待队列上的进程重新放回到工作队列，该进程变成运行状态(当socket事件触发时，也就是有数据到来，会取下一个进程结构调用其回调，将其挂到工作队列中)，继续执行代码。也由于socket的接收缓冲区已经有了数据，recv可以返回接收到的数据。

4.内核接收网络数据全过程

+ 操作系统如何知道网络数据对应于哪个socket？
  + 因为一个socket对应着一个端口号，而网络数据包中包含了ip和端口的信息，内核可以通过端口号找到对应的socket。当然，为了提高处理速度，操作系统会维护端口号到socket的索引结构，以快速读取。(就是说网卡中断CPU后，CPU的中断函数从网卡存数据的内存拷贝数据到对应fd的接收缓冲区，具体是哪一个fd，CPU会检查port，放到对应的fd中）
+ 如何同时监视多个socket的数据？
  + 多路复用

5.同时监视多个socket的简单方法

+ 服务器端需要管理多个客户端连接，而recv只能监视单个socket，这种矛盾下，人们开始寻找监视多个socket的方法。epoll的要义是高效的监视多个socket。从历史发展角度看，必然先出现一种不太高效的方法，人们再加以改进。只有先理解了不高效的方法，才能够理解epoll的本质。

+ 假如能够预先传入一个socket列表，如果列表中的socket都没有数据，挂起进程，直到一个socket接收到数据2，唤醒进程。 这种方法很直接，也是select的设计思想。

+ 为了方便理解，我们先复习select的用法。在如下的代码中，先准备一个数组(下面代码中的fds)，让fds存放着所有需要监视的socket。然后调用select，如果fds中的所有socket都没有数据，select会阻塞，直到有一个(也可以是多个)socket接收到数据，select返回，唤醒进程。用户可以遍历fds，通过FD_ISSET判断具体哪一个socket收到数据，然后做出处理。

  ~~~c
  int s = socket(AF_INET, SOCK_STREAM, 0);  
  bind(s, ...)
  listen(s, ...)
   
  int fds[] =  存放需要监听的socket
   
  while(1){
      int n = select(..., fds, ...)
      for(int i=0; i < fds.count; i++){
          if(FD_ISSET(fds[i], ...)){
              //fds[i]的数据处理
          }
      }
  }
  ~~~

+ select的流程

  + select的实现思路很直接。假如程序同时监视sock1，sock2和sock3三个socket，那么在调用select之后，操作系统把进程A(包括了select逻辑)分别加入这三和socket的等待队列中。
  + 操作系统把基础A分别加入这三个socket的等待队列中，当任何一个socket收到数据后，中断程序将唤醒进程。所谓唤醒进程，就是将进程从所有的等待队列中移除，加入到工作队列里面。当进程A被唤醒后，它知道至少有一个socket接收了数据。程序只需遍历以便 socket列表，就可以得到就绪的socket。

+ select的缺点

  + 每次调用select都需要将进程加入到所有监视socket的等待队列，每次唤醒都需要从每个队列中移除。这里涉及两次遍历(遍历进程A关心的所有socket，需要注意的是从等待队列头部添加，删除通过回调直接实现，所以每个socket的等待队列不用遍历)，而且每次都要将这fds列表传递给内核，有一定的开销，真是因为遍历操作开销大，出于效率的考量，才会规定select的最大监视数量，默认只能监视1024个socket。
  + 进程被唤醒后，进程并不知道那些socket收到数据，还需要遍历一次(这一次遍历是在应用层)。
  + 那么有没有减少遍历的方法呢？有没有保存就绪socket的方法？这两个问题便是epoll技术要解决的。当程序调用select时，内核会先遍历一遍socket，如果有一个以上的socket接收缓冲区有数据，那么select直接返回，不会阻塞。这也是为什么select的返回值有可能大于1的原因之一。如果没有socket有数据，进程才会阻塞。

+ epoll的设计思路

  + epoll是在select出现N多年后才被发明的，是selcet和poll的增强版本。epoll通过以下一些措施来改进效率。
  + 措施一：功能分离
    + select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。每次调用select都需要添加等待队列和阻塞进程两个步骤，然而大多数应用场景中，需要监视的socket相对固定，并不需要每次都修改。epoll将这两个操作分开，先用epoll_ctl维护等待队列，在调用epoll_wait阻塞进程(解耦)。显而易见的，效率就能得到提升。
    + 

